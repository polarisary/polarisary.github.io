<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Scrapy整体抓取流程 | 默刀</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/7.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-113714645-1','auto');ga('send','pageview');</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Scrapy整体抓取流程</h1><a id="logo" href="/.">默刀</a><p class="description">Study as if you were to live forever, live as if you were to die tomorrow.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Scrapy整体抓取流程</h1><div class="post-meta">Jun 6, 2018<span> | </span><span class="category"><a href="/categories/源码研究/">源码研究</a></span></div><div class="post-content"><p>看代码过程中整理的一张思维导图，对理解整个流程有帮助。<br><img src="/2018/06/06/scrapy-flow/1.jpg" alt="Scrapy抓取流程"></p>
<p>从启动示例说起：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy.crawler import CrawlerProcess</span><br><span class="line"></span><br><span class="line">class MySpider1(scrapy.Spider):</span><br><span class="line">    # Your first spider definition</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">class MySpider2(scrapy.Spider):</span><br><span class="line">    # Your second spider definition</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">process = CrawlerProcess()</span><br><span class="line">process.crawl(MySpider1)</span><br><span class="line">process.crawl(MySpider2)</span><br><span class="line">process.start() # the script will block here until all crawling jobs are finished</span><br></pre></td></tr></table></figure></p>
<p>这是官方推荐的在一个进程启动多个Spider的示例；先实例化一个CrawlerProcess实例，这个就是一个Scrapy进程，接着添加两个Spider，并且启动进程，看起来很简单，下面结合源代码分析Scrapy是怎么完成抓取的。</p>
<p>CrawlerProcess继承自CrawlerRunner，上面示例中CrawlerProcess实例化后，调用crawl方法添加Spider，看看具体代码执行了什么？<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def crawl(self, crawler_or_spidercls, *args, **kwargs):</span><br><span class="line">     crawler = self.create_crawler(crawler_or_spidercls)</span><br><span class="line">     return self._crawl(crawler, *args, **kwargs)</span><br><span class="line"> def _crawl(self, crawler, *args, **kwargs):</span><br><span class="line">     self.crawlers.add(crawler)</span><br><span class="line">     d = crawler.crawl(*args, **kwargs)</span><br><span class="line">     self._active.add(d)</span><br><span class="line">     def _done(result):</span><br><span class="line">         self.crawlers.discard(crawler)</span><br><span class="line">         self._active.discard(d)</span><br><span class="line">         return result</span><br><span class="line">     return d.addBoth(_done)</span><br></pre></td></tr></table></figure></p>
<p>可以看出这里是用参数Spider创建了个crawler，并且调用crawler的crawl方面。顺藤摸瓜看看Crawler.crawl()方法到底干了什么？<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">@defer.inlineCallbacks</span><br><span class="line">def crawl(self, *args, **kwargs):</span><br><span class="line">    assert not self.crawling, &quot;Crawling already taking place&quot;</span><br><span class="line">    self.crawling = True</span><br><span class="line">    try:</span><br><span class="line">        self.spider = self._create_spider(*args, **kwargs)</span><br><span class="line">        self.engine = self._create_engine()</span><br><span class="line">        start_requests = iter(self.spider.start_requests())</span><br><span class="line">        yield self.engine.open_spider(self.spider, start_requests)</span><br><span class="line">        yield defer.maybeDeferred(self.engine.start)</span><br><span class="line">    except Exception:</span><br><span class="line">        if six.PY2:</span><br><span class="line">            exc_info = sys.exc_info()</span><br><span class="line">        self.crawling = False</span><br><span class="line">        if self.engine is not None:</span><br><span class="line">            yield self.engine.close()</span><br><span class="line">        if six.PY2:</span><br><span class="line">            six.reraise(*exc_info)</span><br><span class="line">        raise</span><br></pre></td></tr></table></figure></p>
<p>这里创建Spider，创建engine。接着调用engine.open_spider(),engine.start()，这个流程先打住，待会回过头来再接下来分析这里。先看下示例代码最后一步process.start()。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def start(self, stop_after_crawl=True):</span><br><span class="line">    if stop_after_crawl:</span><br><span class="line">        d = self.join()</span><br><span class="line">        # Don&apos;t start the reactor if the deferreds are already fired</span><br><span class="line">        if d.called:</span><br><span class="line">            return</span><br><span class="line">        d.addBoth(self._stop_reactor)</span><br><span class="line">    reactor.installResolver(self._get_dns_resolver())</span><br><span class="line">    tp = reactor.getThreadPool()</span><br><span class="line">    tp.adjustPoolsize(maxthreads=self.settings.getint(&apos;REACTOR_THREADPOOL_MAXSIZE&apos;))</span><br><span class="line">    reactor.addSystemEventTrigger(&apos;before&apos;, &apos;shutdown&apos;, self.stop)</span><br><span class="line">    reactor.run(installSignalHandlers=False)  # blocking call</span><br></pre></td></tr></table></figure></p>
<p>这里引入了Twisted的事件循环并启动，之后上面的engine会注册相关的方法到事件循环中执行。接着上面engine分析。看看open_spider()干了什么？start()很简单，这里不介绍。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">@defer.inlineCallbacks</span><br><span class="line">def open_spider(self, spider, start_requests=(), close_if_idle=True):</span><br><span class="line">    assert self.has_capacity(), &quot;No free spider slot when opening %r&quot; % \</span><br><span class="line">        spider.name</span><br><span class="line">    logger.info(&quot;Spider opened&quot;, extra=&#123;&apos;spider&apos;: spider&#125;)</span><br><span class="line">    nextcall = CallLaterOnce(self._next_request, spider)</span><br><span class="line">    scheduler = self.scheduler_cls.from_crawler(self.crawler)</span><br><span class="line">    start_requests = yield self.scraper.spidermw.process_start_requests(start_requests, spider)</span><br><span class="line">    slot = Slot(start_requests, close_if_idle, nextcall, scheduler)</span><br><span class="line">    self.slot = slot</span><br><span class="line">    self.spider = spider</span><br><span class="line">    yield scheduler.open(spider)</span><br><span class="line">    yield self.scraper.open_spider(spider)</span><br><span class="line">    self.crawler.stats.open_spider(spider)</span><br><span class="line">    yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)</span><br><span class="line">    slot.nextcall.schedule()</span><br><span class="line">    slot.heartbeat.start(5)</span><br></pre></td></tr></table></figure></p>
<p>这里创建调度器Scheduler，并调用Spider中间件管理器注册的中间件的process_start_requests对start_requests做相应处理。重点是nextcall，这个就是向上面主流程中的事件循环中注册事件的。从中可以看出把_next_request方法注册到时间循环，并且没5秒钟执行一次。并且这个nextcall也是可以主动调度的。</p>
<p>接着分析engine._next_request()的实现：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def _next_request_from_scheduler(self, spider):</span><br><span class="line">       slot = self.slot</span><br><span class="line">       request = slot.scheduler.next_request()</span><br><span class="line">       if not request:</span><br><span class="line">           return</span><br><span class="line">       d = self._download(request, spider)</span><br><span class="line">       d.addBoth(self._handle_downloader_output, request, spider)</span><br><span class="line">       d.addErrback(lambda f: logger.info(&apos;Error while handling downloader output&apos;,</span><br><span class="line">                                          exc_info=failure_to_exc_info(f),</span><br><span class="line">                                          extra=&#123;&apos;spider&apos;: spider&#125;))</span><br><span class="line">       d.addBoth(lambda _: slot.remove_request(request))</span><br><span class="line">       d.addErrback(lambda f: logger.info(&apos;Error while removing request from slot&apos;,</span><br><span class="line">                                          exc_info=failure_to_exc_info(f),</span><br><span class="line">                                          extra=&#123;&apos;spider&apos;: spider&#125;))</span><br><span class="line">       d.addBoth(lambda _: slot.nextcall.schedule())</span><br><span class="line">       d.addErrback(lambda f: logger.info(&apos;Error while scheduling new request&apos;,</span><br><span class="line">                                          exc_info=failure_to_exc_info(f),</span><br><span class="line">                                          extra=&#123;&apos;spider&apos;: spider&#125;))</span><br><span class="line">       return d</span><br></pre></td></tr></table></figure>
<p>先从调度器中获取一个请求，执行下载，这中间要经过下载中间件层层过滤。接着注册回调函数_handle_downloader_output方法处理下载后的结果。<br>看看_handle_downloader_output做了什么：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def _handle_downloader_output(self, response, request, spider):</span><br><span class="line">       assert isinstance(response, (Request, Response, Failure)), response</span><br><span class="line">       if isinstance(response, Request):</span><br><span class="line">           self.crawl(response, spider)</span><br><span class="line">           return</span><br><span class="line">       # response is a Response or Failure</span><br><span class="line">       d = self.scraper.enqueue_scrape(response, request, spider)</span><br><span class="line">       d.addErrback(lambda f: logger.error(&apos;Error while enqueuing downloader output&apos;,</span><br><span class="line">                                           exc_info=failure_to_exc_info(f),</span><br><span class="line">                                           extra=&#123;&apos;spider&apos;: spider&#125;))</span><br><span class="line">       return d</span><br></pre></td></tr></table></figure></p>
<p>如果返回结果是Request类型，需要重新调用crawl()方法，具体做法是，先交给调度器调度。如果返回类型是Response或者Failure，则交给scraper处理。接着看看enqueue_scrape()干了啥？<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def enqueue_scrape(self, response, request, spider):</span><br><span class="line">       slot = self.slot</span><br><span class="line">       dfd = slot.add_response_request(response, request)</span><br><span class="line">       def finish_scraping(_):</span><br><span class="line">           slot.finish_response(response, request)</span><br><span class="line">           self._check_if_closing(spider, slot)</span><br><span class="line">           self._scrape_next(spider, slot)</span><br><span class="line">           return _</span><br><span class="line">       dfd.addBoth(finish_scraping)</span><br><span class="line">       dfd.addErrback(</span><br><span class="line">           lambda f: logger.error(&apos;Scraper bug processing %(request)s&apos;,</span><br><span class="line">                                  &#123;&apos;request&apos;: request&#125;,</span><br><span class="line">                                  exc_info=failure_to_exc_info(f),</span><br><span class="line">                                  extra=&#123;&apos;spider&apos;: spider&#125;))</span><br><span class="line">       self._scrape_next(spider, slot)</span><br><span class="line">       return dfd</span><br></pre></td></tr></table></figure></p>
<p>在scraper内部也会维护一个队列，其中add_response_request就是队列的producer，往队列里放任务，_scrape_next则是队列consumer，消费队列的任务。看看如何消费？<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def _scrape(self, response, request, spider):</span><br><span class="line">       &quot;&quot;&quot;Handle the downloaded response or failure through the spider</span><br><span class="line">       callback/errback&quot;&quot;&quot;</span><br><span class="line">       assert isinstance(response, (Response, Failure))</span><br><span class="line"></span><br><span class="line">       dfd = self._scrape2(response, request, spider) # returns spiders processed output</span><br><span class="line">       dfd.addErrback(self.handle_spider_error, request, response, spider)</span><br><span class="line">       dfd.addCallback(self.handle_spider_output, request, response, spider)</span><br><span class="line">       return dfd</span><br><span class="line">def _scrape2(self, request_result, request, spider):</span><br><span class="line">       if not isinstance(request_result, Failure):</span><br><span class="line">           return self.spidermw.scrape_response(</span><br><span class="line">               self.call_spider, request_result, request, spider)</span><br><span class="line">       else:</span><br><span class="line">           # FIXME: don&apos;t ignore errors in spider middleware</span><br><span class="line">           dfd = self.call_spider(request_result, request, spider)</span><br><span class="line">           return dfd.addErrback(</span><br><span class="line">               self._log_download_errors, request_result, request, spider)</span><br></pre></td></tr></table></figure></p>
<p>如果是Response类型，在执行Spider中间件的scrape_response方法，并注册Spider结果处理函数handle_spider_output();看看handle_spider_output干了些什么？<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def handle_spider_output(self, result, request, response, spider):</span><br><span class="line">       if not result:</span><br><span class="line">           return defer_succeed(None)</span><br><span class="line">       it = iter_errback(result, self.handle_spider_error, request, response, spider)</span><br><span class="line">       dfd = parallel(it, self.concurrent_items,</span><br><span class="line">           self._process_spidermw_output, request, response, spider)</span><br><span class="line">       return dfd</span><br><span class="line">def _process_spidermw_output(self, output, request, response, spider):</span><br><span class="line">       if isinstance(output, Request):</span><br><span class="line">           self.crawler.engine.crawl(request=output, spider=spider)</span><br><span class="line">       elif isinstance(output, (BaseItem, dict)):</span><br><span class="line">           self.slot.itemproc_size += 1</span><br><span class="line">           dfd = self.itemproc.process_item(output, spider)</span><br><span class="line">           dfd.addBoth(self._itemproc_finished, output, response, spider)</span><br><span class="line">           return dfd</span><br><span class="line">       elif output is None:</span><br><span class="line">           pass</span><br><span class="line">       else:</span><br><span class="line">           typename = type(output).__name__</span><br><span class="line">           logger.error(&apos;Spider must return Request, BaseItem, dict or None, &apos;</span><br><span class="line">                        &apos;got %(typename)r in %(request)s&apos;,</span><br><span class="line">                        &#123;&apos;request&apos;: request, &apos;typename&apos;: typename&#125;,</span><br><span class="line">                        extra=&#123;&apos;spider&apos;: spider&#125;)</span><br></pre></td></tr></table></figure></p>
<p>首先在deffer中注册_process_spidermw_output方法，在_process_spidermw_output的处理中，如果接收到的结果是Request，就通知engine抓取此请求。如果接收到的是BaseItem或者dict类型的数据，则调用配置的itempipeline的process_item方法，这里往往是数据存DB或者写文件中，到此整理流程也就完成了。这只是正常流程的大概描述，中间还有很多异常处理和状态监控、log等等。</p>
</div><div class="tags"><a href="/tags/python/">python</a><a href="/tags/Scrapy/">Scrapy</a><a href="/tags/源码/">源码</a></div><div class="post-nav"><a class="next" href="/2018/06/03/growthhacker/">《增长黑客》读后感</a></div><div id="container"></div><link rel="stylesheet" href="/css/default.css?v=0.0.0"><script src="/js/gitment.browser.js?v=0.0.0"></script><script>var gitment = new Gitment({
  owner: 'polarisary',
  repo: 'polarisary.github.io',
  oauth: {
    client_id: 'cc1c26fba7cb369cd10b',
    client_secret: '3300c5f614b65a2505151ad18b7268ac0997986b',
  },
})
gitment.render('container')
</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><iframe width="100%" height="120" style="margin-bottom: 20px;" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=400&fansRow=1&ptype=1&speed=0&skin=1&isTitle=1&noborder=1&isWeibo=1&isFans=1&uid=2029904011&verifier=8ceb4535&dpc=1"></iframe><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://polarisary.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/HTTP/">HTTP</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/IO/">IO</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MySQL/">MySQL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Unity/">Unity</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/产品-运营/">产品&运营</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/分布式/">分布式</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/源码研究/">源码研究</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/打包/" style="font-size: 15px;">打包</a> <a href="/tags/总结/" style="font-size: 15px;">总结</a> <a href="/tags/多列索引/" style="font-size: 15px;">多列索引</a> <a href="/tags/单列索引/" style="font-size: 15px;">单列索引</a> <a href="/tags/MVCC/" style="font-size: 15px;">MVCC</a> <a href="/tags/增长黑客/" style="font-size: 15px;">增长黑客</a> <a href="/tags/Growth-Hacker/" style="font-size: 15px;">Growth Hacker</a> <a href="/tags/用户增长/" style="font-size: 15px;">用户增长</a> <a href="/tags/B-Tree/" style="font-size: 15px;">B-Tree</a> <a href="/tags/HTTP/" style="font-size: 15px;">HTTP</a> <a href="/tags/TCP-IP/" style="font-size: 15px;">TCP/IP</a> <a href="/tags/加密/" style="font-size: 15px;">加密</a> <a href="/tags/Blocking/" style="font-size: 15px;">Blocking</a> <a href="/tags/IO/" style="font-size: 15px;">IO</a> <a href="/tags/同步/" style="font-size: 15px;">同步</a> <a href="/tags/异步/" style="font-size: 15px;">异步</a> <a href="/tags/分布式/" style="font-size: 15px;">分布式</a> <a href="/tags/系统设计/" style="font-size: 15px;">系统设计</a> <a href="/tags/pip/" style="font-size: 15px;">pip</a> <a href="/tags/MySQL/" style="font-size: 15px;">MySQL</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/冒泡/" style="font-size: 15px;">冒泡</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Web/" style="font-size: 15px;">Web</a> <a href="/tags/Scrapy/" style="font-size: 15px;">Scrapy</a> <a href="/tags/源码/" style="font-size: 15px;">源码</a> <a href="/tags/Spider/" style="font-size: 15px;">Spider</a> <a href="/tags/Scrapyd/" style="font-size: 15px;">Scrapyd</a> <a href="/tags/unity/" style="font-size: 15px;">unity</a> <a href="/tags/shader/" style="font-size: 15px;">shader</a> <a href="/tags/渲染流水线/" style="font-size: 15px;">渲染流水线</a> <a href="/tags/光照模型/" style="font-size: 15px;">光照模型</a> <a href="/tags/快排/" style="font-size: 15px;">快排</a> <a href="/tags/Zen/" style="font-size: 15px;">Zen</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/TC/" style="font-size: 15px;">TC</a> <a href="/tags/QOS/" style="font-size: 15px;">QOS</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/06/06/scrapy-flow/">Scrapy整体抓取流程</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/03/growthhacker/">《增长黑客》读后感</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/30/scrapyd-deploy/">使用Scrapyd部署爬虫</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/16/io-model/">IO模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/13/pangu/">阿里云分布式存储系统的研究与分享</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/27/tc/">Linux TC限流</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/02/08/python-web/">Python Web流程</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/02/02/http/">HTTP协议简介</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/31/MySQL索引补充/">MySQL单列索引和多列索引</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/23/MySql索引/">MySql索引</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://coolshell.cn/" title="CoolShell" target="_blank">CoolShell</a><ul></ul><a href="http://ifeve.com/" title="并发编程" target="_blank">并发编程</a><ul></ul><a href="http://www.raychase.net/" title="四火的唠叨" target="_blank">四火的唠叨</a><ul></ul><a href="http://blog.chenpeng.info/" title="陈鹏个人博客" target="_blank">陈鹏个人博客</a><ul></ul><a href="http://www.coderli.com/" title="OneCoder" target="_blank">OneCoder</a><ul></ul><a href="http://vimer.me/" title="Vimer.me" target="_blank">Vimer.me</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">默刀.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>